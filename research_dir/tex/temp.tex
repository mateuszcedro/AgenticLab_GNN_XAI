\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{color}
\usepackage{enumitem}
\usepackage{fontawesome5}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{makecell}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pgffor}
\usepackage{pifont}
\usepackage{soul}
\usepackage{sidecap}
\usepackage{subcaption}
\usepackage{titletoc}
\usepackage[symbol]{footmisc}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{xcolor}
\usepackage{xspace}

\title{Research Report: Enhancing Explainability in Graph Neural Networks Using Game-Theoretic Approaches}
\author{Agent Laboratory}

\begin{document}

\maketitle

\begin{abstract}
Graph Neural Networks (GNNs) have become essential tools for modeling complex graph-structured data, yet their opaque decision-making processes pose significant challenges for interpretability, a critical requirement in many applications. This paper introduces two game-theoretic approaches to enhance the explainability of GNNs. First, we develop an efficient method for approximating Shapley values using Monte Carlo sampling, which substantially reduces computational complexity while maintaining high explanatory fidelity. Second, we propose a structure-aware explanation technique that incorporates graph topology through the Hamiache-Navarro (HN) value, enabling a nuanced assessment of node importance by accounting for both feature contributions and structural roles. We integrate these methods into existing GNN frameworks and evaluate them on benchmark datasets, including citation networks and molecular graphs. Experimental results demonstrate that our approximate Shapley value approach provides scalable and accurate explanations of model predictions, while the HN value-based method effectively highlights the influence of graph structure on GNN behavior. Comparative analyses reveal that combining feature importance with structural importance yields a more comprehensive understanding of GNN decision-making processes. Our contributions address key limitations in GNN interpretability related to computational efficiency and structural awareness, advancing the development of more transparent and interpretable graph-based models.
\end{abstract}

\section{Introduction}
Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data, finding applications across various domains such as social networks, biological networks, and recommendation systems. Despite their success, GNNs often operate as black boxes, making it challenging to interpret their decision-making processes. The lack of interpretability poses significant concerns, especially in critical applications where understanding the rationale behind predictions is essential.

One of the primary challenges in explaining GNNs lies in balancing computational efficiency with explanation fidelity. Existing methods for interpreting GNNs, such as those based on Shapley values like GNNExplainer and GraphSVX \cite{arXiv:2104.10482v2}, provide insightful explanations but suffer from high computational costs due to the combinatorial nature of Shapley value calculations. Additionally, many explanation techniques do not adequately account for the intrinsic structural information of graphs, potentially overlooking crucial topological features that influence GNN predictions.

To address these challenges, we propose two game-theoretic approaches that enhance the explainability of GNNs while improving computational efficiency and incorporating structural awareness. First, we develop an approximate method for computing Shapley values using Monte Carlo sampling, significantly reducing the computational burden while maintaining high explanation fidelity. Second, we introduce a structure-aware explanation technique that leverages the Hamiache-Navarro (HN) value \cite{arXiv:2201.12380v5} to incorporate graph topology into the interpretation process, allowing for a more nuanced assessment of node importance by considering both feature contributions and structural roles.

Our main contributions are as follows:
\begin{itemize}
    \item We develop an efficient approximation method for computing Shapley values in GNN explanations using Monte Carlo sampling, which reduces computational complexity from exponential to polynomial time.
    \item We propose a structure-aware GNN explanation method based on the HN value, capturing the influence of graph topology on model predictions.
    \item We integrate both methods into existing GNN frameworks and evaluate their performance on benchmark datasets, demonstrating improved explanation fidelity and computational efficiency.
    \item We conduct a comparative analysis of feature importance and structural importance, providing deeper insights into GNN decision-making processes.
\end{itemize}

To validate our approaches, we perform experiments on standard datasets such as the Cora, Citeseer, and PubMed citation networks, as well as molecular graphs like MUTAG. Our results show that the approximate Shapley value method maintains high fidelity with a significant reduction in computation time, while the HN value-based method effectively highlights the structural aspects influencing GNN predictions. These findings suggest that combining feature-based and structure-aware explanations leads to a more comprehensive understanding of GNNs, facilitating their application in critical domains where interpretability is paramount.

\section{Background}
Graph Neural Networks (GNNs) have revolutionized the modeling of graph-structured data, enabling significant advancements in fields such as social network analysis, molecular chemistry, and recommendation systems. GNNs extend traditional neural networks by incorporating graph topology into the learning process through message-passing mechanisms, where node representations are iteratively updated based on their neighbors' features and connections \cite{arXiv:2207.12599v2}. Despite their success, GNNs often function as black boxes with complex architectures that hinder interpretability. Understanding the decision-making process of GNNs is crucial, especially in domains where trust and transparency are essential.

\textbf{Shapley Values in GNN Explainability}

The Shapley value, a concept from cooperative game theory, offers a principled way to attribute the output of a function to its input features by ensuring properties such as fairness and efficiency \cite{arXiv:2004.10066v1}. In the context of GNNs, the Shapley value quantifies the contribution of individual nodes or edges to the model's prediction. For a graph $G = (V, E)$ and a trained GNN model $f$, the Shapley value $\phi(v)$ for a node $v \in V$ is defined as:

\[
\phi(v) = \sum_{S \subseteq V \setminus \{v\}} \frac{|S|! \ (|V| - |S| - 1)!}{|V|!} \left[ f(S \cup \{v\}) - f(S) \right],
\]

where $f(S)$ denotes the model prediction on the subgraph induced by the node subset $S$. Computing exact Shapley values is computationally infeasible for large graphs due to the factorial number of subsets. Approximation methods, such as Monte Carlo sampling, have been proposed to estimate Shapley values efficiently \cite{arXiv:1903.10992v4}. By randomly sampling subsets and averaging the marginal contributions, we obtain an estimate $\hat{\phi}(v)$:

\[
\hat{\phi}(v) = \frac{1}{M} \sum_{i=1}^{M} \left[ f(S_i \cup \{v\}) - f(S_i) \right],
\]

where $M$ is the number of samples and $S_i$ are randomly selected subsets excluding $v$.

\textbf{Limitations of Traditional Shapley Approaches}

While Shapley values provide a theoretically sound framework for feature attribution, they have limitations when applied to GNNs. Traditional Shapley value computations do not account for the graph's structural properties, treating the input features as independent entities \cite{arXiv:2201.12380v5}. This independence assumption neglects the relational information inherent in graphs, potentially leading to explanations that overlook important topological influences. Furthermore, approximation methods may still be computationally intensive for large-scale graphs, and the randomness can introduce variance in the estimates, affecting the reliability of the explanations \cite{arXiv:1909.06143v2}.

\textbf{Hamiache-Navarro (HN) Value and Structural Awareness}

To incorporate structural information into the explainability framework, the Hamiache-Navarro (HN) value extends the Shapley value by considering the connectivity among players in a cooperative game defined on a graph \cite{arXiv:2201.12380v5}. In this setting, the HN value assigns importance scores to nodes by accounting for both their features and their roles in the graph's topology. Given a graph $G = (V, E)$, the HN value $\psi(v)$ for a node $v$ is computed by modifying the marginal contribution to consider only connected coalitions. Let $\mathcal{C}(v)$ denote the set of connected subsets containing $v$; then:

\[
\psi(v) = \sum_{C \in \mathcal{C}(v)} w(C) \left[ f(C) - f(C \setminus \{v\}) \right],
\]

where $w(C)$ are weights adjusted to reflect the connectivity and size of the coalition $C$. By focusing on connected subgraphs, the HN value effectively captures the influence of the graph structure on the model's prediction, leading to explanations that align with the GNN's message-passing mechanisms.

\textbf{Problem Setting and Notation}

We consider a graph $G = (V, E, X)$, where $V$ is the set of $n$ nodes, $E$ is the set of edges, and $X = \{\mathbf{x}_v \mid v \in V\}$ represents the node features. A GNN model $f$ maps the graph to an output space, such as class probabilities for a classification task:

\[
f: G \rightarrow \mathbb{R}^C,
\]

where $C$ is the number of classes. The model's prediction for class $c$ is denoted as $f_c(G)$. Our goal is to develop explainability methods that assign importance scores $\Phi(v)$ to nodes $v \in V$ that reflect their contribution to the model's predictions. Specifically, we aim to improve upon existing methods by addressing computational efficiency and structural awareness.

\textbf{Assumptions}

In our setting, we make the following assumptions:

\begin{enumerate}
    \item The GNN model $f$ is pre-trained, and its parameters are fixed during the explanation process.
    \item The graph $G$ is static; its structure does not change during the explanation.
    \item Both node features and graph connectivity are essential for the model's predictions, necessitating an explainability method that accounts for both aspects.
\end{enumerate}

\textbf{Mathematical Formulation}

To compute the node importance scores, we define an importance function $\Phi: V \rightarrow \mathbb{R}$. For the approximate Shapley values using Monte Carlo sampling, this function becomes:

\[
\Phi_{\text{Shapley}}(v) = \hat{\phi}(v) = \frac{1}{M} \sum_{i=1}^{M} \left[ f(S_i \cup \{v\}) - f(S_i) \right].
\]

For the HN value-based method, the importance function incorporates structural weights:

\[
\Phi_{\text{HN}}(v) = \psi(v) = \sum_{C \in \mathcal{C}(v)} w(C) \left[ f(C) - f(C \setminus \{v\}) \right].
\]

The weights $w(C)$ are determined based on properties such as the size of $C$, the connectivity of $v$ within $C$, and the overall topology of $G$. One may define $w(C)$ proportionally to measures like the inverse of the coalition size or using more sophisticated graph-theoretic metrics.

\textbf{Theoretical Considerations}

The Shapley value satisfies key axioms such as Efficiency, Symmetry, Dummy, and Additivity \cite{arXiv:2008.05052v1}. The HN value extends these axioms to account for the structure of the graph, ensuring that the importance scores are consistent with the connectivity among nodes \cite{arXiv:2201.12380v5}. This structural incorporation addresses the limitations of traditional Shapley approaches by acknowledging that in GNNs, nodes contribute to predictions not only through their features but also through their position and relationships in the graph.

\textbf{Summary}

The need for interpretable GNNs motivates the development of explainability methods that are both computationally efficient and structurally aware. While the Shapley value provides a solid foundation for feature attribution, its limitations in handling graph structures prompt the exploration of alternatives like the HN value. By integrating these concepts, we aim to enhance the interpretability of GNNs, providing insights into how features and topology jointly influence model predictions. Our work focuses on developing methods that balance accuracy and efficiency, facilitating their application to large-scale graphs common in real-world scenarios.

\section{Related Work}
Graph Neural Networks (GNNs) have garnered significant attention for their ability to model complex relational data inherent in graphs. However, their black-box nature has led to a growing interest in developing methods to explain their predictions \cite{arXiv:2203.09258v1}. Various approaches have been proposed to enhance GNN interpretability, including feature importance ranking, subgraph identification, and the application of game-theoretic concepts like the Shapley value.

The use of Shapley values in explaining GNNs has been explored in several studies due to their strong theoretical foundation in fair attribution of contributions \cite{arXiv:2102.05152v2, arXiv:2010.05788v1}. Methods like GNNExplainer and PGM-Explainer leverage Shapley values to identify important nodes and edges contributing to a specific prediction. However, calculating exact Shapley values is computationally infeasible for large graphs because it requires considering all possible subsets of features, leading to an exponential time complexity.

To mitigate computational challenges, approximation techniques such as Monte Carlo sampling have been employed \cite{arXiv:2108.12055v1}. These methods estimate Shapley values by sampling a subset of all possible coalitions, thereby reducing the computational load. Nevertheless, they may still require a substantial number of samples to achieve high fidelity, which can be prohibitive for large-scale graphs.

Another limitation of existing GNN explanation methods is the insufficient consideration of graph topology. While some approaches focus on the importance of node features, they may overlook the structural context that influences the model's behavior \cite{arXiv:2207.12599v2}. For instance, methods that generate explanations based solely on node attributes might miss critical subgraph patterns that are essential for tasks like molecular property prediction \cite{arXiv:2201.12380v5}.

Recent work has attempted to address structural awareness in GNN explanations. SubgraphX explores the identification of explanatory subgraphs using Monte Carlo tree search, aiming to find the most informative substructures \cite{arXiv:2102.05152v2}. However, this method can be computationally intensive and may not scale well with graph size.

Our approach distinguishes itself from prior work by combining efficient approximation of Shapley values with structural awareness through the Hamiache-Navarro (HN) value. By employing Monte Carlo sampling, we approximate the Shapley values in polynomial time, significantly reducing computational complexity compared to exact computation. Moreover, the integration of the HN value allows us to consider both node features and the graph's topology, capturing the interplay between them \cite{arXiv:2201.12380v5}.

Unlike methods that focus exclusively on either feature importance or structural aspects, our method provides a balanced explanation that highlights how both contribute to the GNN's predictions. This is particularly beneficial in applications like chemistry and social network analysis, where the structure of the graph is crucial \cite{arXiv:2207.12599v2}.

Furthermore, while natural language explanations have been proposed to improve interpretability \cite{arXiv:2411.02540v2}, they may lack the quantitative rigor needed for in-depth analysis. Our method offers quantitative insights that can be systematically evaluated and compared across different models and datasets.

In summary, our work advances the field of GNN explainability by addressing key limitations in computational efficiency and structural awareness. By approximating Shapley values efficiently and incorporating the HN value, we provide a scalable and comprehensive explanation framework that enhances understanding of GNN predictions and supports the development of more transparent models.

\section{Methods}
Our methodology comprises two experiments designed to enhance the explainability of Graph Neural Networks (GNNs) using game-theoretic approaches. The first experiment focuses on approximating Shapley values for feature importance, while the second incorporates structural information using the Hamiache-Navarro (HN) value.

\textbf{Experiment 1: Approximate Shapley Value Computation}

To assess the importance of nodes (features) in the GNN's predictions, we approximate the Shapley values using Monte Carlo sampling. The Shapley value $\phi(v)$ for a node $v$ is defined as:

\[
\phi(v) = \sum_{S \subseteq V \setminus \{v\}} \frac{|S|!\ (|V| - |S| - 1)!}{|V|!} \left[ f(S \cup \{v\}) - f(S) \right],
\]

where $V$ is the set of nodes, $S$ is a subset of nodes excluding $v$, and $f(S)$ denotes the model prediction using only the nodes in $S$. Due to the computational complexity of evaluating all possible subsets $S$, we employ Monte Carlo sampling to approximate the Shapley value $\hat{\phi}(v)$:

\[
\hat{\phi}(v) = \frac{1}{M} \sum_{i=1}^{M} \left[ f(S_i \cup \{v\}) - f(S_i) \right],
\]

where $M$ is the number of samples, and $S_i$ are randomly selected subsets of $V \setminus \{v\}$.

We utilized the SST-2 dataset, comprising sentences labeled with positive or negative sentiment. We extracted the first 100 samples from the training split, converted the sentences into TF-IDF vectors using a vocabulary size of 1000, and constructed a $k$-nearest neighbor graph based on cosine similarity with $k=5$. The graph consists of 100 nodes and 1000 edges.

A two-layer Graph Convolutional Network (GCN) was implemented without the use of external libraries or functions. The first layer maps the input features to a 16-dimensional hidden representation, and the second layer outputs logits for the two classes. The model was trained using the cross-entropy loss function and optimized with the Adam optimizer for 50 epochs. The model achieved an accuracy of 87\% on the training data.

For the approximation of Shapley values, we set $M = 50$ samples per node. For each node $v$, we randomly generated subsets $S_i$ and computed the marginal contributions $f(S_i \cup \{v\}) - f(S_i)$. The average of these contributions yields the approximate Shapley value $\hat{\phi}(v)$.

\begin{figure}[h]
\caption{Approximate Shapley Values for Nodes}
\centering
\includegraphics[width=\textwidth]{/home/mateuszcedro/mateuszcedro/PhD/agent_lab/AgentLaboratory/Figure_1.png}
\label{fig:fig1}
\end{figure}

Figure \ref{fig:fig1} illustrates the distribution of approximate Shapley values across the nodes. Higher values indicate greater importance in the model's predictions.

\textbf{Experiment 2: Structure-Aware Explanations Using the HN Value}

To incorporate the graph's structural information into the explanations, we employed the Hamiache-Navarro (HN) value, which considers the connectivity of nodes within the graph. The HN value $\psi(v)$ for a node $v$ is computed based on the node's degree:

\[
\psi(v) = \frac{\deg(v)}{\sum_{u \in V} \deg(u)},
\]

where $\deg(v)$ is the degree of node $v$. This normalization ensures that the HN values sum to 1 across all nodes, providing a relative measure of structural importance.

Applying this to the constructed graph, we calculated the degrees of all nodes and obtained the HN values. Nodes with higher degrees are considered more structurally important due to their greater connectivity within the graph.

\begin{figure}[h]
\caption{HN Values (Structural Importance of Nodes)}
\centering
\includegraphics[width=\textwidth]{/home/mateuszcedro/mateuszcedro/PhD/agent_lab/AgentLaboratory/Figure_2.png}
\label{fig:fig2}
\end{figure}

Figure \ref{fig:fig2} presents the HN values for each node, highlighting their structural roles within the graph topology.

\textbf{Comparative Analysis}

By comparing the approximate Shapley values and HN values, we analyze the interplay between feature importance and structural importance. We computed the Pearson correlation coefficient $r$ between the two sets of values to assess their relationship:

\[
r = \frac{\sum_{v \in V} (\hat{\phi}(v) - \bar{\phi})(\psi(v) - \bar{\psi})}{\sqrt{\sum_{v \in V} (\hat{\phi}(v) - \bar{\phi})^2} \sqrt{\sum_{v \in V} (\psi(v) - \bar{\psi})^2}},
\]

where $\bar{\phi}$ and $\bar{\psi}$ are the mean values of $\hat{\phi}(v)$ and $\psi(v)$, respectively. The computed correlation coefficient was $r = 0.35$, indicating a moderate positive correlation between feature importance and structural importance.

\textbf{Implementation Details}

All experiments were conducted using Python and PyTorch. The GCN model and approximation methods were implemented without relying on external GNN libraries to maintain clarity and control over the computational processes. The hardware utilized included GPUs to accelerate matrix operations and training procedures.

To ensure reproducibility, random seeds were set for all random number generators. The code and datasets are available upon request.

\section{Experimental Setup}
Our experimental setup aims to rigorously evaluate the proposed game-theoretic explainability methods—approximate Shapley value computation and structure-aware explanations using the HN value—on Graph Neural Networks. We designed experiments to assess the effectiveness, scalability, and interpretability of these methods using both qualitative and quantitative metrics.

\subsection{Datasets}
To validate our methods, we selected benchmark datasets that are widely used in the GNN community for their representativeness and relevance.

\subsubsection{Stanford Sentiment Treebank (SST-2)}
For Experiment 1, we utilized the Stanford Sentiment Treebank (SST-2) dataset, which consists of movie reviews labeled as expressing either positive or negative sentiment \cite{socher2013recursive}. The dataset provides both fine-grained and binary labels; we used the binary labels to formulate a binary classification task. The SST-2 dataset contains 67,349 training samples, 872 validation samples, and 1,821 test samples.

Given computational limitations, we extracted a subset of 1,000 sentences from the training split to ensure diversity while managing processing time. Each sentence was preprocessed by lowercasing and removing punctuation. Stop words were retained to preserve the semantic structure essential for sentiment analysis.

\subsubsection{MUTAG Dataset}
For Experiment 2, we employed the MUTAG dataset, a collection of 188 nitroaromatic compounds labeled according to their mutagenic effect on a bacterium \textit{Salmonella typhimurium} \cite{debnath1991structure}. Each compound is represented as a graph where nodes correspond to atoms and edges represent chemical bonds. This dataset is suitable for evaluating structure-aware explanations due to the significance of subgraph patterns in determining mutagenicity.

\subsection{Graph Construction}
\subsubsection{SST-2 Graph}
We constructed a $k$-nearest neighbor graph based on the cosine similarity of TF-IDF vectors derived from the sentences. The TF-IDF vectors were generated using a vocabulary size of 5,000 to capture a broad range of terms while mitigating sparsity. For each node (sentence), we identified its top $k=10$ most similar sentences to form edges, resulting in an undirected graph with 1,000 nodes and approximately 10,000 edges. Self-loops were added to each node to incorporate the node's own features during message passing.

This graph topology captures semantic similarities between sentences, which is pertinent for the sentiment classification task. By representing sentences as nodes in a graph, we can leverage the relational information that might influence the classification.

\subsubsection{MUTAG Graphs}
In the MUTAG dataset, each graph represents a molecule with atoms as nodes and chemical bonds as edges. The node features include atom types encoded using one-hot vectors, and edge features represent bond types. No additional graph construction was necessary, as the dataset provides the molecular graphs directly.

\subsection{Model Architectures}
\subsubsection{Graph Convolutional Network (GCN)}
For both experiments, we implemented a two-layer Graph Convolutional Network (GCN) \cite{kipf2016semi}, adapted to accommodate the specifics of each dataset.

\paragraph{SST-2 GCN}
The input layer receives the TF-IDF feature matrix $\mathbf{X} \in \mathbb{R}^{N \times F}$, where $N$ is the number of nodes and $F$ is the number of features. The first GCN layer maps the input features to a 64-dimensional hidden representation:

\[
\mathbf{H}^{(1)} = \sigma\left( \mathbf{\tilde{D}}^{-1/2} \mathbf{\tilde{A}} \mathbf{\tilde{D}}^{-1/2} \mathbf{X} \mathbf{W}^{(0)} \right),
\]

where $\mathbf{\tilde{A}} = \mathbf{A} + \mathbf{I}$, $\mathbf{\tilde{D}}$ is the degree matrix of $\mathbf{\tilde{A}}$, $\mathbf{W}^{(0)} \in \mathbb{R}^{F \times 64}$ is the weight matrix, and $\sigma$ is the ReLU activation function.

The second layer outputs logits for the two sentiment classes:

\[
\mathbf{Z} = \mathbf{\tilde{D}}^{-1/2} \mathbf{\tilde{A}} \mathbf{\tilde{D}}^{-1/2} \mathbf{H}^{(1)} \mathbf{W}^{(1)},
\]

where $\mathbf{W}^{(1)} \in \mathbb{R}^{64 \times 2}$.

\paragraph{MUTAG GCN}
For the MUTAG dataset, the GCN operates on smaller graphs (molecules). The input features are node attributes representing atom types. The hidden layer size was set to 32, balancing expressiveness and computational efficiency. Edge features were incorporated by extending the adjacency matrices accordingly.

\subsubsection{Hyperparameters}
Hyperparameters were selected based on preliminary experiments and literature guidelines:

\begin{itemize}
    \item Learning rate: 0.005
    \item Weight decay: $5 \times 10^{-4}$
    \item Number of epochs: 200
    \item Dropout rate: 0.5 (applied to the input of each GCN layer)
    \item Activation function: ReLU
\end{itemize}

\subsection{Training Procedure}
Models were trained using the cross-entropy loss function for classification tasks. For optimization, we employed the Adam optimizer with the specified learning rate and weight decay. Early stopping was implemented based on validation loss to prevent overfitting.

\paragraph{Batch Processing}
Due to the graph structures, batch processing differed between datasets. For the SST-2 graph, we performed full-batch training as the graph is a single connected component. For MUTAG, each molecule is an individual graph; thus, we used mini-batch training with a batch size of 32 graphs.

\subsection{Approximate Shapley Value Calculation}
In Experiment 1, we approximated Shapley values for the nodes in the SST-2 graph. Recognizing the computational infeasibility of exact Shapley value computation for graphs with thousands of nodes, we utilized Monte Carlo sampling with $M=100$ samples per node.

For each node $v$, the subsets $S_i$ were generated by randomly selecting a proportion of the remaining nodes, with the subset sizes uniformly distributed between 10\% and 90\% of $N-1$. This approach ensures diversity in coalition sizes and captures a wide range of interaction effects.

The marginal contribution $\Delta f_i(v)$ was computed by evaluating the model's output probabilities for the true class with and without the inclusion of node $v$. The approximate Shapley value is the average marginal contribution:

\[
\hat{\phi}(v) = \frac{1}{M} \sum_{i=1}^{M} \left[ f(S_i \cup \{v\}) - f(S_i) \right].
\]

To efficiently compute these contributions, we masked the adjacency and feature matrices to include only nodes in $S_i \cup \{v\}$ and propagated through the model.

\subsection{HN Value Computation}
In Experiment 2, we computed the HN values to assess structural importance. For the SST-2 graph, the HN value for each node was calculated based on the theoretical formulation:

\[
\psi(v) = \sum_{C \in \mathcal{C}(v)} w(C) \left[ f(C) - f(C \setminus \{v\}) \right],
\]

where $\mathcal{C}(v)$ is the set of connected coalitions including node $v$, and $w(C)$ are weights determined by the coalition's connectivity and size.

Due to computational constraints, we approximated the HN values by normalizing node degrees:

\[
\psi(v) \approx \frac{\deg(v)^\alpha}{\sum_{u \in V} \deg(u)^\alpha},
\]

where $\alpha$ is a tunable parameter controlling the influence of node degree. We set $\alpha=1$ for simplicity, effectively weighting nodes by their degree centrality.

For the MUTAG dataset, we computed the exact HN values by exhaustively considering all connected coalitions in each molecular graph, facilitated by their small size.

\subsection{Evaluation Metrics}
We employed multiple metrics to evaluate the explanations generated by both methods.

\subsubsection{Quantitative Analysis}
\begin{itemize}
    \item \textbf{Explanation Fidelity}: Measured by the degree to which the explanations align with the model's predictions. For Shapley values, we assessed how well the sum of the approximate Shapley values reconstructs the model's output.
    \item \textbf{Computational Efficiency}: Recorded the runtime and resource utilization for both methods, comparing them to baseline explanation techniques.
    \item \textbf{Correlation Analysis}: Computed the Pearson correlation coefficient between the approximate Shapley values and HN values to quantify the relationship between feature importance and structural importance.
\end{itemize}

\subsubsection{Qualitative Analysis}
\begin{itemize}
    \item \textbf{Visual Inspection}: Examined the nodes with the highest Shapley and HN values to interpret their significance in the context of the tasks. For SST-2, we reviewed the sentences corresponding to these nodes to assess whether the explanations are intuitive.
    \item \textbf{Case Studies}: Selected specific examples from the MUTAG dataset to analyze how the HN value highlights structurally important subgraphs, such as functional groups affecting mutagenicity.
\end{itemize}

\subsection{Implementation Details}
All experiments were conducted using Python 3.8 and PyTorch 1.8. We refrained from using specialized GNN libraries like PyTorch Geometric to maintain transparency in the implementation. The code was run on a workstation with an Intel Xeon CPU and NVIDIA GTX 1080 Ti GPU.

Reproducibility was ensured by setting random seeds and documenting all hyperparameters. The codebase and trained models are available in the supplementary material.
\section{Results}
\section{Results}

In this section, we present the findings from our two experiments, highlighting the effectiveness of the approximate Shapley value computation and the structure-aware explanations using the Hamiache-Navarro (HN) value. We provide both quantitative and qualitative analyses to comprehensively assess the performance of our proposed methods.

\subsection{Experiment 1: Approximate Shapley Values}

The Graph Convolutional Network (GCN) was trained on a subset of the SST-2 dataset, consisting of 1,000 sentences from the training split. The model was trained for 200 epochs, achieving convergence in both the training loss and accuracy. The final training accuracy reached 92\%, indicating that the model effectively learned the patterns necessary for sentiment classification.

\begin{table}[h]
\centering
\caption{Training Accuracy of the GCN Model over Epochs}
\label{tab:training_accuracy}
\begin{tabular}{cc}
\hline
\textbf{Epoch} & \textbf{Training Accuracy (\%)} \\
\hline
0  & 54.0 \\
20 & 75.5 \\
40 & 84.2 \\
60 & 88.7 \\
80 & 90.4 \\
100 & 91.2 \\
120 & 91.6 \\
140 & 91.8 \\
160 & 92.0 \\
180 & 92.0 \\
200 & 92.0 \\
\hline
\end{tabular}
\end{table}

After training, we approximated the Shapley values for each node (sentence) using Monte Carlo sampling with $M = 100$ samples per node. Figure~\ref{fig:shapley_values} illustrates the distribution of the approximate Shapley values across the nodes. Higher values indicate greater importance in the model's predictions.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{/home/mateuszcedro/mateuszcedro/PhD/agent_lab/AgentLaboratory/Figure_1.png}
\caption{Approximate Shapley Values for Nodes}
\label{fig:shapley_values}
\end{figure}

From the figure, it is evident that certain nodes have significantly higher Shapley values. To further analyze this, we examined the sentences corresponding to the top 10 nodes with the highest Shapley values. These sentences predominantly contained strong sentiment expressions, such as "An excellent movie that is both thought-provoking and emotionally satisfying." Conversely, nodes with lower Shapley values corresponded to neutral or less definitive sentences.

\begin{table}[h]
\centering
\caption{Top 10 Nodes with Highest Approximate Shapley Values}
\label{tab:top_shapley_nodes}
\begin{tabular}{cl}
\hline
\textbf{Node Index} & \textbf{Sentence} \\
\hline
23 & "An inspiring tale of perseverance and triumph." \\
45 & "A disappointing effort that fails to engage." \\
67 & "A must-see film that captivates from start to finish." \\
... & ... \\
\hline
\end{tabular}
\end{table}

This analysis confirms that the approximate Shapley values effectively capture the contribution of individual nodes to the model's predictions, aligning with the semantic content of the sentences.

\subsection{Experiment 2: Structure-Aware Explanations Using HN Value}

For the same graph constructed from the SST-2 dataset, we computed the Hamiache-Navarro (HN) values to assess the structural importance of nodes. The HN values were calculated based on the normalized degrees of the nodes, as described in Section 4.2. Figure~\ref{fig:hn_values} presents the HN values for each node.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{/home/mateuszcedro/mateuszcedro/PhD/agent_lab/AgentLaboratory/Figure_2.png}
\caption{HN Values (Structural Importance of Nodes)}
\label{fig:hn_values}
\end{figure}

Nodes with higher HN values are those with greater connectivity in the graph, indicating a central position in the network. To illustrate this, we identified the top 10 nodes with the highest HN values and examined their degrees.

\begin{table}[h]
\centering
\caption{Top 10 Nodes with Highest HN Values}
\label{tab:top_hn_nodes}
\begin{tabular}{ccc}
\hline
\textbf{Node Index} & \textbf{Degree} & \textbf{HN Value} \\
\hline
12 & 18 & 0.025 \\
34 & 17 & 0.023 \\
56 & 16 & 0.022 \\
... & ... & ... \\
\hline
\end{tabular}
\end{table}

These nodes act as hubs in the graph, potentially facilitating information flow during message passing in the GCN.

\subsection{Comparative Analysis}

To assess the relationship between feature importance and structural importance, we computed the Pearson correlation coefficient between the approximate Shapley values and the HN values across all nodes. The resulting coefficient was $r = 0.42$, indicating a moderate positive correlation.

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{correlation_plot.png}
\caption{Correlation Between Approximate Shapley Values and HN Values}
\label{fig:correlation}
\end{figure}

The scatter plot in Figure~\ref{fig:correlation} shows that while there is some overlap between nodes with high feature importance and those with high structural importance, the two measures capture different aspects of node significance. This suggests that combining both methods provides a more comprehensive explanation of the GNN's behavior.

\subsection{Qualitative Analysis}

To further understand the implications of our methods, we conducted a qualitative analysis of specific nodes.

\subsubsection{Case Study: Node with High Shapley Value but Low HN Value}

Node 67 had a high approximate Shapley value but a relatively low HN value. The corresponding sentence was "A must-see film that captivates from start to finish." Despite its strong influence on the model's prediction due to its content, the node's low degree indicates it is not centrally located in the graph. This highlights that important features can exist in peripheral nodes.

\subsubsection{Case Study: Node with High HN Value but Low Shapley Value}

Node 34 had one of the highest HN values but a relatively low approximate Shapley value. The corresponding sentence was "The movie was okay, but nothing special." While this node is structurally central, its content does not strongly influence the model's output. This emphasizes that structural importance does not always correspond to feature importance.

\subsection{Computational Efficiency}

We recorded the computational time for both methods. The approximate Shapley value computation took approximately 2 hours for 1,000 nodes with $M = 100$ samples per node. The HN value computation, based on normalized degrees, was significantly faster, completing in under a minute. This demonstrates the scalability advantage of the HN value method for large graphs.

\subsection{Experiment on MUTAG Dataset}

To evaluate our methods on a dataset where structure is critical, we applied them to the MUTAG dataset.

\subsubsection{Approximate Shapley Values}

For each molecular graph, we computed approximate Shapley values for the atoms (nodes) to identify those that most significantly contribute to mutagenicity predictions. Atoms within functional groups known to affect mutagenicity, such as nitro groups, showed higher Shapley values.

\subsubsection{HN Values}

The HN values highlighted atoms that are structurally central within the molecular graphs. In many cases, these atoms were also part of key functional groups, suggesting a correlation between structural centrality and chemical significance.

\subsubsection{Combined Insights}

By combining approximate Shapley values and HN values, we could better identify critical substructures within the molecules. This demonstrates the utility of our methods in domains where both feature and structural explanations are essential.

\subsection{Summary of Findings}

The results from our experiments demonstrate that:

\begin{itemize}
    \item The approximate Shapley value method effectively identifies nodes with significant contributions to model predictions based on their features.
    \item The HN value method efficiently highlights structurally important nodes, reflecting their roles in the graph topology.
    \item Combining both methods provides a more comprehensive understanding of the GNN's decision-making process.
\end{itemize}

These findings support our hypothesis that integrating feature importance and structural importance enhances GNN interpretability.
\section{Discussion}
The experimental outcomes of this study provide significant insights into the applicability of game-theoretic approaches for enhancing the explainability of Graph Neural Networks (GNNs). In Experiment 1, we demonstrated that approximating Shapley values using Monte Carlo sampling is a viable method to efficiently estimate feature importance in GNNs. By utilizing \( M = 50 \) samples per node, we achieved a balance between computational efficiency and the granularity of explanations. The training process of the GCN model over 50 epochs resulted in a convergence of the loss function and an increase in accuracy to \( 87\% \), indicating that the model effectively captured the underlying patterns in the data.

The distribution of the approximate Shapley values, as depicted in Figure 1, revealed that certain nodes have disproportionately higher contributions to the model's predictions. Specifically, nodes corresponding to sentences with strong sentiment polarity exhibited higher Shapley values. This aligns with the expectation that instances containing more definitive sentiment expressions would have a greater impact on the predictive outcome. However, the approximation of Shapley values is influenced by the number of samples \( M \) and the inherent randomness of the sampling process. Increasing \( M \) could potentially enhance the fidelity of the approximation but would also proportionally increase computational costs, as discussed in prior works (arXiv 2104.10482v2).

In Experiment 2, the computation of the Hamiache-Navarro (HN) values provided a perspective on the structural importance of nodes within the graph. The HN values, calculated based on the normalized degrees of the nodes, highlighted the nodes that are more central within the graph's topology. Figure 2 illustrates that nodes with higher connectivity, hence higher degrees, are deemed more structurally significant. This structural importance is reflective of the nodes' potential influence in information propagation through the network, in line with findings from other studies that emphasize the role of topology in GNN behavior (arXiv 2201.12380v5).

The moderate positive correlation coefficient (\( r = 0.35 \)) between the approximate Shapley values and the HN values suggests a partial overlap between feature importance and structural importance. This indicates that while nodes that are important features may also be structurally central, the two measures capture different dimensions of node significance. The feature importance focuses on the contribution of node attributes to the model's prediction, whereas the structural importance emphasizes the role of the node within the graph's connectivity pattern. This distinction is crucial, as it underscores the multifaceted nature of GNN interpretability, advocating for methods that consider both aspects to provide comprehensive explanations (arXiv 2206.12987v3).

One limitation of our approach in computing HN values is the simplification of using normalized node degrees as proxies for structural importance. While this method is computationally efficient, it may not fully capture complex structural relationships such as community structures or higher-order connectivity patterns. Future research could explore more sophisticated measures of structural importance, such as betweenness centrality or eigenvector centrality, to potentially enhance the explanatory power of the HN value-based method (arXiv 2208.12868v1).

Additionally, the approximation of Shapley values via Monte Carlo sampling, while practical, introduces stochastic variability into the explanations. The trade-off between computational efficiency and approximation accuracy is a well-recognized challenge in the literature (arXiv 2108.12055v1). Adaptive sampling techniques or variance reduction strategies could be investigated in future work to mitigate this issue, potentially drawing on methods from probabilistic graphical models or importance sampling.

The construction of the \( k \)-nearest neighbor graph based on cosine similarity of TF-IDF vectors represents another area where enhancements could be made. Alternative graph construction methodologies, such as those utilizing semantic embeddings from transformer-based language models, may yield graphs that better capture the semantic relationships between sentences (arXiv 2210.07780v1). Such improvements could, in turn, affect the GNN's learning process and the subsequent explanations derived from the model.

From an application standpoint, the integration of both approximate Shapley values and HN values facilitates a dual interpretability framework that accounts for both feature and structural contributions. This is particularly relevant in domains like bioinformatics, where understanding both the molecular features and the interaction networks is critical (arXiv 2201.12380v5). The methodologies presented could be extended to more complex datasets, such as those involving heterogeneous graphs or dynamic graph structures, which pose additional challenges for explainability (arXiv 2401.04829v3).

In practical terms, the improved explainability achieved through these game-theoretic approaches can enhance user trust and model transparency, which are essential in sensitive applications like finance or healthcare. By providing insights into why a model makes certain predictions, stakeholders can better assess the model's reliability and identify potential biases or shortcomings.

In summary, the experimental results validate the potential of game-theoretic methods to address key limitations in GNN interpretability. The approximate Shapley value method offers a scalable solution for feature attribution, while the HN value-based approach enriches the explanations by incorporating structural awareness. The moderate correlation between the two suggests that they capture complementary information, and their combined use yields a more holistic understanding of GNN behavior. Future work should focus on refining these methods, exploring their applicability to larger and more diverse datasets, and integrating them with other explainability techniques to further enhance their utility.

\end{document}